{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langgraph langchain-openai langchain-community dotenv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from typing import TypedDict, Annotated\n",
    "from langgraph.graph import add_messages, StateGraph, END\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "search_tool = TavilySearchResults(max_results=2)\n",
    "tools = [search_tool]\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "llm_with_tools = llm.bind_tools(tools=tools)\n",
    "\n",
    "def model(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [llm_with_tools.invoke(state[\"messages\"])], \n",
    "    }\n",
    "\n",
    "def tools_router(state: AgentState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "\n",
    "    if(hasattr(last_message, \"tool_calls\") and len(last_message.tool_calls) > 0):\n",
    "        return \"tool_node\"\n",
    "    else: \n",
    "        return END\n",
    "    \n",
    "\n",
    "tool_node = ToolNode(tools=tools)\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"model\", model)\n",
    "graph.add_node(\"tool_node\", tool_node)\n",
    "graph.set_entry_point(\"model\")\n",
    "\n",
    "graph.add_conditional_edges(\"model\", tools_router)\n",
    "graph.add_edge(\"tool_node\", \"model\")\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\"What's the current weather in Bangalore?\"]\n",
    "}\n",
    "\n",
    "events = app.stream(input=input, stream_mode=\"values\")\n",
    "\n",
    "for event in events: \n",
    "    print(event[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\"Hi, how are you?\"]\n",
    "}\n",
    "\n",
    "events = app.astream_events(input=input, version=\"v2\")\n",
    "\n",
    "async for event in events: \n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "    \"messages\": [\"Hi, how are you?\"]\n",
    "}\n",
    "\n",
    "events = app.astream_events(input=input, version=\"v2\")\n",
    "\n",
    "async for event in events: \n",
    "    if event[\"event\"] == \"on_chat_model_stream\":\n",
    "        print(event[\"data\"][\"chunk\"].content, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
